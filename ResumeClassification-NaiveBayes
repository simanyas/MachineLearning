import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.metrics import accuracy_score
from pandas.plotting import scatter_matrix
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfVectorizer
from matplotlib.gridspec import GridSpec
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
import string
from wordcloud import WordCloud

df = pd.read_csv('UpdatedResumeDataSet.csv')
df.head()
# Display the distinct categories of resume
df.Category.unique()

# Display the distinct categories of resume and the number of records belonging to each category
df.Category.value_counts()

sns.set(rc={'figure.figsize':(15,10)})
sns.countplot(x=df.Category,data=df)
plt.xticks(rotation=290)

sns.countplot(y=df.Category,data=df)

df_pie = df.Category.value_counts().reset_index()
df_pie.columns = ['Category','Counts']
myexplode = [0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0]
colors = sns.color_palette('coolwarm')
plt.figure(1, figsize=(10,10))
#the_grid = GridSpec(1, 1)
# plt.subplot

plt.pie(df_pie.Counts,labels=df_pie.Category, autopct='%0.1f%%',explode = myexplode, colors = colors)
plt.show()

def convert_to_lower(text):
  return text.lower()

df.Resume = df.Resume.apply(convert_to_lower)
df.tail()

import re
def cleanResume(resume_text):
  resume_text = re.sub('\S+:\/\/\S+\s*', ' ', resume_text)  # remove URLs like http://, https://, ftp:// etc. Basically search for pattern "://"
  resume_text = re.sub('www\S+', ' ', resume_text)  # remove URLs which start with "www"
  resume_text = re.sub('RT|cc', ' ', resume_text)  # remove RT and cc
  resume_text = re.sub('#\S+', ' ', resume_text)  # remove hashtags
  resume_text = re.sub('@\S+', ' ', resume_text)  # remove mentions
  resume_text = re.sub('[^A-Za-z0-9 ]+', ' ', resume_text)  # remove non word characters
  resume_text = re.sub('\s+', ' ', resume_text)  # remove extra whitespace
  return resume_text

df['Cleaned_Resume'] = df.Resume.apply(cleanResume)
df

sent_lens = []
for  i in df.Cleaned_Resume:
    length = len(i.split())
    sent_lens.append(length)
    
print("Number of Resumes",len(sent_lens))
print("Longest Resume", max(sent_lens))

# stemming the text
def simple_stemmer(text):
    ps=nltk.porter.PorterStemmer()
    text= ' '.join([ps.stem(word) for word in text.split()])
    return text
# apply function on Cleaned_Resume column
df['Cleaned_Resume'] = df['Cleaned_Resume'].apply(simple_stemmer)

# setting english stopwords
stopword_list = nltk.corpus.stopwords.words('english')
print(stopword_list)

# removing the stopwords
def remove_stopwords(text, is_lower_case=False):
    # splitting strings into tokens (list of words)
    tokens = word_tokenize(text)
    tokens = [token.strip() for token in tokens]
    if is_lower_case:
        # filtering out the stop words
        filtered_tokens = [token for token in tokens if token not in stopword_list]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)    
    return filtered_text

# apply function on review column
df['Cleaned_Resume'] = df['Cleaned_Resume'].apply(remove_stopwords)

from nltk.probability import FreqDist
df_resume =  df.Cleaned_Resume
resume_text = ' '.join(resume_content for resume_content in df.Cleaned_Resume)
fdist = FreqDist()
for sentence in nltk.tokenize.sent_tokenize(resume_text):
  for word in nltk.tokenize.word_tokenize(sentence):
    fdist[word] += 1

fdist

plt.figure(figsize=(15,15))
WC = WordCloud(width=600, height=600, max_words=200, min_font_size=5)
resume_words = WC.generate_from_frequencies(fdist)
plt.imshow(resume_words, interpolation='bilinear')
plt.show
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(df.Category)
LabelEncoder()
list(le.classes_)
df['Category_Encoded'] = le.transform(df.Category)

list(le.inverse_transform([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]))

X_train, X_test, y_train, y_test = train_test_split(df.Cleaned_Resume, df.Category_Encoded, test_size = 0.2, random_state = 42,shuffle=True)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# tfidf vectorizer
tv = TfidfVectorizer(max_df=0.8, use_idf=True, ngram_range = (1,3),max_features=20000)

#transformed train reviews
tfidf_train_resumes = tv.fit_transform(X_train)

#transformed test reviews
tfidf_test_resumes = tv.transform(X_test)

print('Tfidf_train:', tfidf_train_resumes.shape)
print(len(tv.get_feature_names()))
print('Tfidf_test:', tfidf_test_resumes.shape)

# training the model
mnb=MultinomialNB(alpha=0.75, fit_prior=True)

# fitting the NaiveBayes for tfidf features
mnb_tfidf = mnb.fit(tfidf_train_resumes, y_train)
print('MultinomialNB for tf-idf :',mnb_tfidf)

# predicting the model for tfidf features
mnb_tfidf_predict = mnb.predict(tfidf_test_resumes)
print('predictions for tf-idf :', mnb_tfidf_predict)

from sklearn.metrics import confusion_matrix
# accuracy score for tf-idf
mnb_tfidf_score = accuracy_score(y_test, mnb_tfidf_predict)
print("mnb_tfidf_score :", mnb_tfidf_score)

# confusion matrix for tf-idf
cm_tfidf = confusion_matrix(y_test,mnb_tfidf_predict)
print('confusion matrix for tf-idf :\n', cm_tfidf)

!pip -qq install gradio
import gradio
# Function for preprocessing of text
def preprocess_text(text):
    text = convert_to_lower(text)
    text = cleanResume(text)
    text = simple_stemmer(text)
    text = remove_stopwords(text)
    return text
    
# Function to predict label for a review
def predict_resume_category(text):
    processed_text = preprocess_text(text)
    resume_transformed = tv.transform([processed_text])
    pred = mnb_tfidf.predict(resume_transformed)
    return pred[0]

# Input from user
in_resume = gradio.inputs.Textbox(lines=2, placeholder=None, default="resume", label='Enter Resume')

# Output prediction
out_label = gradio.outputs.Textbox(type="auto", label="Predicted Resume Category: '0:Advocate','1:Arts','2:AutomationTesting','3:Blockchain','4:Business Analyst','5:Civil Engineer','6:Data Science','7:Database','8:DevOpsEngineer','9:DotNet Developer','10:ETL Developer','11:Electrical Engineering','12:HR','13:Hadoop','14:Health and fitness','15:Java Developer','16:Mechanical Engineer','17:Network Security Engineer','18:Operations Manager','19:PMO','20:Python Developer','21:SAP Developer','22:Sales','23:Testing','24:Web Designing'")

# Gradio interface to generate UI link
iface = gradio.Interface(
  fn = predict_resume_category,
  inputs = [in_resume],
  outputs = [out_label])
iface.launch(share=True)
